{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, to_date, when\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Initialize Spark session - This starts our PySpark environment\n",
    "spark = SparkSession.builder.appName(\"DataPreprocessing\").getOrCreate()\n",
    "\n",
    "# Define file locations in cloud storage\n",
    "file_location_cards = \"/FileStore/tables/cards_data.csv\"\n",
    "file_location_users = \"/FileStore/tables/users_data.csv\"\n",
    "file_location_amex = \"/FileStore/tables/amex_filtered_transactions.csv\"\n",
    "\n",
    "# Step 1: Load the datasets from cloud storage\n",
    "# Cards data contains card details like credit limit and PIN change info\n",
    "df_cards = spark.read.csv(file_location_cards, header=True, inferSchema=True)\n",
    "# Users data has user info like income and age\n",
    "df_users = spark.read.csv(file_location_users, header=True, inferSchema=True)\n",
    "# Amex filtered transactions from our previous work\n",
    "df_amex_transactions = spark.read.csv(file_location_amex, header=True, inferSchema=True)\n",
    "\n",
    "# Step 2: Preprocess cards data (kept as DataFrame)\n",
    "# Drop the 'card_on_dark_web' column since we don’t need it\n",
    "df_cards = df_cards.drop(\"card_on_dark_web\")\n",
    "# Clean 'credit_limit' by removing '$' and converting to float\n",
    "df_cards = df_cards.withColumn(\n",
    "    \"credit_limit\",\n",
    "    regexp_replace(col(\"credit_limit\"), \"[\\$,]\", \"\").cast(FloatType())\n",
    ")\n",
    "# Convert 'acct_open_date' to a proper date format (MM/YYYY)\n",
    "df_cards = df_cards.withColumn(\n",
    "    \"acct_open_date\",\n",
    "    to_date(col(\"acct_open_date\"), \"MM/yyyy\")\n",
    ")\n",
    "# Flag if a PIN change is due - if last changed over 5(not update) + 2 years years ago\n",
    "current_year = 2025  # Using 2025 based on your context\n",
    "df_cards = df_cards.withColumn(\n",
    "    \"PIN_Change_Due\",\n",
    "    when(col(\"year_pin_last_changed\") < current_year - 7, \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "# Step 3: Preprocess users data (kept as DataFrame)\n",
    "# Clean financial columns by removing '$' and converting to float\n",
    "df_users = df_users.withColumn(\n",
    "    \"per_capita_income\",\n",
    "    regexp_replace(col(\"per_capita_income\"), \"[\\$,]\", \"\").cast(FloatType())\n",
    ").withColumn(\n",
    "    \"yearly_income\",\n",
    "    regexp_replace(col(\"yearly_income\"), \"[\\$,]\", \"\").cast(FloatType())\n",
    ").withColumn(\n",
    "    \"total_debt\",\n",
    "    regexp_replace(col(\"total_debt\"), \"[\\$,]\", \"\").cast(FloatType())\n",
    ")\n",
    "# Add 'retirement_status' - Retired if current age meets or exceeds retirement age\n",
    "df_users = df_users.withColumn(\n",
    "    \"retirement_status\",\n",
    "    when(col(\"current_age\") >= col(\"retirement_age\"), \"Retired\").otherwise(\"Not Retired\")\n",
    ")\n",
    "# Categorize 'age_group' based on current age\n",
    "df_users = df_users.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"current_age\") <= 30, \"18-30\")\n",
    "    .when(col(\"current_age\") <= 45, \"31-45\")\n",
    "    .when(col(\"current_age\") <= 60, \"46-60\")\n",
    "    .otherwise(\"60+\")\n",
    ")\n",
    "# Calculate 'Debt_to_Income_Ratio' as total debt divided by yearly income\n",
    "df_users = df_users.withColumn(\n",
    "    \"Debt_to_Income_Ratio\",\n",
    "    col(\"total_debt\") / col(\"yearly_income\")\n",
    ")\n",
    "\n",
    "# Step 4: Preprocess amex_filtered_transactions (kept as DataFrame)\n",
    "# Clean 'amount' by removing '$' and converting to float\n",
    "df_amex_transactions = df_amex_transactions.withColumn(\n",
    "    \"amount\",\n",
    "    regexp_replace(col(\"amount\"), \"[\\$,]\", \"\").cast(FloatType())\n",
    ")\n",
    "\n",
    "# Step 5: Show a preview of the preprocessed DataFrames\n",
    "# Let’s peek at the first few rows to confirm our changes\n",
    "print(\"Preview of preprocessed cards data:\")\n",
    "df_cards.show(5, truncate=False)\n",
    "print(\"Preview of preprocessed users data:\")\n",
    "df_users.show(5, truncate=False)\n",
    "print(\"Preview of preprocessed Amex transactions (with cleaned amount):\")\n",
    "df_amex_transactions.show(5, truncate=False)\n",
    "\n",
    "# Step 6: Count rows and confirm we’re done\n",
    "# We’ll count the rows in each DataFrame to see what we’re working with\n",
    "cards_count = df_cards.count()\n",
    "users_count = df_users.count()\n",
    "amex_count = df_amex_transactions.count()\n",
    "print(f\"Cards DataFrame has {cards_count} rows\")\n",
    "print(f\"Users DataFrame has {users_count} rows\")\n",
    "print(f\"Amex Transactions DataFrame has {amex_count} rows\")\n",
    "\n",
    "# Note: Spark session remains active since we’re keeping DataFrames in memory\n",
    "# Use spark.stop() later when you’re done with these DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LOGISTIC REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import dagshub\n",
    "dagshub.init(repo_owner='mayankpareek740', repo_name='aihi', mlflow=True)\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/mayankpareek740/aihi.mlflow\")\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import FloatType\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# UDF to extract Class 1 probability\n",
    "def get_class1_prob(prob):\n",
    "    return float(prob[1])\n",
    "get_class1_prob_udf = udf(get_class1_prob, FloatType())\n",
    "\n",
    "# Data joining (same as original)\n",
    "amex = df_amex_transactions.alias(\"amex\")\n",
    "cards = df_cards.alias(\"cards\")\n",
    "users = df_users.alias(\"users\")\n",
    "\n",
    "df_temp = amex.join(\n",
    "    cards,\n",
    "    amex[\"card_id\"] == cards[\"id\"],\n",
    "    \"left\"\n",
    ").select(\n",
    "    amex[\"id\"].alias(\"transaction_id\"),\n",
    "    amex[\"client_id\"],\n",
    "    amex[\"amount\"],\n",
    "    amex[\"fraud_label\"],\n",
    "    amex[\"mcc\"],\n",
    "    cards[\"credit_limit\"]\n",
    ")\n",
    "\n",
    "df_combined = df_temp.join(\n",
    "    users,\n",
    "    df_temp[\"client_id\"] == users[\"id\"],\n",
    "    \"left\"\n",
    ").select(\n",
    "    df_temp[\"transaction_id\"],\n",
    "    df_temp[\"client_id\"],\n",
    "    df_temp[\"amount\"],\n",
    "    df_temp[\"fraud_label\"],\n",
    "    df_temp[\"mcc\"],\n",
    "    df_temp[\"credit_limit\"],\n",
    "    users[\"gender\"],\n",
    "    users[\"retirement_status\"],\n",
    "    users[\"current_age\"],\n",
    "    users[\"per_capita_income\"],\n",
    "    users[\"yearly_income\"],\n",
    "    usersn[\"total_debt\"],\n",
    "    users[\"Debt_to_Income_Ratio\"]\n",
    ")\n",
    "\n",
    "# Set experiment name\n",
    "mlflow.set_experiment('logisticmodel')\n",
    "\n",
    "# Define feature columns\n",
    "categorical_cols = [\"mcc\", \"gender\", \"retirement_status\"]\n",
    "numerical_cols = [\"amount\", \"current_age\", \"per_capita_income\", \"yearly_income\", \"total_debt\", \"Debt_to_Income_Ratio\", \"credit_limit\"]\n",
    "\n",
    "# Feature preprocessing\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\") for col in categorical_cols]\n",
    "fraud_indexer = StringIndexer(inputCol=\"fraud_label\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "assembler_inputs = [col + \"_index\" for col in categorical_cols] + numerical_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"raw_features\", handleInvalid=\"keep\")\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "\n",
    "# Calculate class weights\n",
    "label_counts = df_combined.groupBy(\"fraud_label\").count()\n",
    "total = df_combined.count()\n",
    "label_weights = label_counts.withColumn(\"weight\", (total / col(\"count\")).cast(\"float\")).select(col(\"fraud_label\"), col(\"weight\"))\n",
    "df_combined = df_combined.join(label_weights, \"fraud_label\", \"left\")\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = df_combined.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Define parameter grid for multiple runs\n",
    "param_grid = [\n",
    "    {\"maxIter\": 50, \"regParam\": 0.1, \"elasticNetParam\": 0.0},\n",
    "    {\"maxIter\": 100, \"regParam\": 0.01, \"elasticNetParam\": 0.5},\n",
    "    {\"maxIter\": 150, \"regParam\": 0.001, \"elasticNetParam\": 1.0}\n",
    "]\n",
    "\n",
    "# Multiple runs with different parameters\n",
    "for i, params in enumerate(param_grid):\n",
    "    with mlflow.start_run(run_name=f\"run_{i+1}\"):\n",
    "        # Create Logistic Regression model\n",
    "        lr = LogisticRegression(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label\",\n",
    "            weightCol=\"weight\",\n",
    "            maxIter=params[\"maxIter\"],\n",
    "            regParam=params[\"regParam\"],\n",
    "            elasticNetParam=params[\"elasticNetParam\"]\n",
    "        )\n",
    "\n",
    "        # Build and fit pipeline\n",
    "        pipeline = Pipeline(stages=indexers + [fraud_indexer, assembler, scaler, lr])\n",
    "        model = pipeline.fit(train_data)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model.transform(test_data)\n",
    "\n",
    "        # Evaluate model\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "        auc = evaluator.evaluate(predictions)\n",
    "        print(f\"Run {i+1} - Area Under ROC: {auc}\")\n",
    "\n",
    "        # Confusion Matrix\n",
    "        confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().collect()\n",
    "        tn = next((row[\"count\"] for row in confusion_matrix if row[\"label\"] == 0.0 and row[\"prediction\"] == 0.0), 0)\n",
    "        fp = next((row[\"count\"] for row in confusion_matrix if row[\"label\"] == 0.0 and row[\"prediction\"] == 1.0), 0)\n",
    "        fn = next((row[\"count\"] for row in confusion_matrix if row[\"label\"] == 1.0 and row[\"prediction\"] == 0.0), 0)\n",
    "        tp = next((row[\"count\"] for row in confusion_matrix if row[\"label\"] == 1.0 and row[\"prediction\"] == 1.0), 0)\n",
    "        print(f\"Run {i+1} - Confusion Matrix:\")\n",
    "        print(f\"TN: {tn} | FP: {fp}\")\n",
    "        print(f\"FN: {fn} | TP: {tp}\")\n",
    "\n",
    "        # Calculate additional metrics\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "        # Log parameters and metrics to MLflow\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"seed\", 42)\n",
    "        mlflow.log_metrics({\n",
    "            \"auc\": auc,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": tp,\n",
    "            \"true_negatives\": tn,\n",
    "            \"false_positives\": fp,\n",
    "            \"false_negatives\": fn\n",
    "        })\n",
    "\n",
    "        # ROC Curve\n",
    "        roc_data = predictions.select(\n",
    "            col(\"label\"),\n",
    "            get_class1_prob_udf(col(\"probability\")).alias(\"prob_class1\")\n",
    "        ).orderBy(col(\"prob_class1\").desc())\n",
    "        \n",
    "        total_positives = tp + fn\n",
    "        total_negatives = tn + fp\n",
    "        roc_points = [(0.0, 0.0)]\n",
    "        cum_tp, cum_fp = 0, 0\n",
    "        for row in roc_data.collect():\n",
    "            if row[\"label\"] == 1.0:\n",
    "                cum_tp += 1\n",
    "            else:\n",
    "                cum_fp += 1\n",
    "            tpr = cum_tp / total_positives if total_positives > 0 else 0\n",
    "            fpr = cum_fp / total_negatives if total_negatives > 0 else 0\n",
    "            roc_points.append((fpr, tpr))\n",
    "\n",
    "        # Plot and save ROC curve\n",
    "        fpr, tpr = zip(*roc_points)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\", color=\"blue\")\n",
    "        plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Guess\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"ROC Curve - Run {i+1}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        roc_file = f\"roc_curve_run_{i+1}.png\"\n",
    "        plt.savefig(roc_file)\n",
    "        mlflow.log_artifact(roc_file)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OUTPUT**\n",
    "- 2025/03/29 06:00:39 INFO mlflow.tracking.fluent: Experiment with name 'logisticmodel' does not exist. Creating a new experiment.\n",
    "\n",
    "## RUN 1\n",
    "- Run 1 - Area Under ROC: 0.6787976710557039\n",
    "- Run 1 - Confusion Matrix:\n",
    "- TN: 94955 | FP: 19311\n",
    "- FN: 89 | TP: 99\n",
    "\n",
    "## RUN 2\n",
    "- Run 2 - Area Under ROC: 0.6733341222105493\n",
    "- Run 2 - Confusion Matrix:\n",
    "- TN: 94922 | FP: 19344\n",
    "- FN: 91 | TP: 97\n",
    "\n",
    "## RUN 3\n",
    "- Run 3 - Area Under ROC: 0.7067603736112564\n",
    "- Run 3 - Confusion Matrix:\n",
    "- TN: 93444 | FP: 20822\n",
    "- FN: 76 | TP: 112"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RAMDOM FOREST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import dagshub\n",
    "dagshub.init(repo_owner='mayankpareek740', repo_name='aihi', mlflow=True)\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/mayankpareek740/aihi.mlflow\")\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import FloatType\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# UDF to extract Class 1 probability\n",
    "def get_class1_prob(prob):\n",
    "    return float(prob[1])\n",
    "get_class1_prob_udf = udf(get_class1_prob, FloatType())\n",
    "\n",
    "# Data joining (same as original)\n",
    "amex = df_amex_transactions.alias(\"amex\")\n",
    "cards = df_cards.alias(\"cards\")\n",
    "users = df_users.alias(\"users\")\n",
    "\n",
    "df_temp = amex.join(\n",
    "    cards,\n",
    "    amex[\"card_id\"] == cards[\"id\"],\n",
    "    \"left\"\n",
    ").select(\n",
    "    amex[\"id\"].alias(\"transaction_id\"),\n",
    "    amex[\"client_id\"],\n",
    "    amex[\"amount\"],\n",
    "    amex[\"fraud_label\"],\n",
    "    amex[\"mcc\"],\n",
    "    cards[\"credit_limit\"]\n",
    ")\n",
    "\n",
    "df_combined = df_temp.join(\n",
    "    users,\n",
    "    df_temp[\"client_id\"] == users[\"id\"],\n",
    "    \"left\"\n",
    ").select(\n",
    "    df_temp[\"transaction_id\"],\n",
    "    df_temp[\"client_id\"],\n",
    "    df_temp[\"amount\"],\n",
    "    df_temp[\"fraud_label\"],\n",
    "    df_temp[\"mcc\"],\n",
    "    df_temp[\"credit_limit\"],\n",
    "    users[\"gender\"],\n",
    "    users[\"retirement_status\"],\n",
    "    users[\"current_age\"],\n",
    "    users[\"per_capita_income\"],\n",
    "    users[\"yearly_income\"],\n",
    "    users[\"total_debt\"],\n",
    "    users[\"Debt_to_Income_Ratio\"]\n",
    ")\n",
    "\n",
    "# Set experiment name\n",
    "mlflow.set_experiment('randomforest')\n",
    "\n",
    "# Define feature columns\n",
    "categorical_cols = [\"mcc\", \"gender\", \"retirement_status\"]\n",
    "numerical_cols = [\"amount\", \"current_age\", \"per_capita_income\", \"yearly_income\", \"total_debt\", \"Debt_to_Income_Ratio\", \"credit_limit\"]\n",
    "\n",
    "# Feature preprocessing\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\") for col in categorical_cols]\n",
    "fraud_indexer = StringIndexer(inputCol=\"fraud_label\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "assembler_inputs = [col + \"_index\" for col in categorical_cols] + numerical_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"raw_features\", handleInvalid=\"keep\")\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "\n",
    "# Calculate class weights\n",
    "label_counts = df_combined.groupBy(\"fraud_label\").count()\n",
    "total = df_combined.count()\n",
    "label_weights = label_counts.withColumn(\"weight\", (total / col(\"count\")).cast(\"float\")).select(col(\"fraud_label\"), col(\"weight\"))\n",
    "df_combined = df_combined.join(label_weights, \"fraud_label\", \"left\")\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = df_combined.randomSplit([0.8, 0.2], seed=24)\n",
    "\n",
    "# Multiple runs with different parameters\n",
    "param_grid = [\n",
    "    {\"numTrees\": 20, \"maxDepth\": 5, \"subsamplingRate\": 0.8},\n",
    "    {\"numTrees\": 50, \"maxDepth\": 10, \"subsamplingRate\": 0.9},\n",
    "    {\"numTrees\": 100, \"maxDepth\": 15, \"subsamplingRate\": 1.0}\n",
    "]\n",
    "\n",
    "for i, params in enumerate(param_grid):\n",
    "    with mlflow.start_run(run_name=f\"run_{i+1}\"):\n",
    "        # Create Random Forest model\n",
    "        rf = RandomForestClassifier(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label\",\n",
    "            weightCol=\"weight\",\n",
    "            numTrees=params[\"numTrees\"],\n",
    "            maxDepth=params[\"maxDepth\"],\n",
    "            subsamplingRate=params[\"subsamplingRate\"],\n",
    "            seed=24\n",
    "        )\n",
    "\n",
    "        # Build and fit pipeline\n",
    "        pipeline = Pipeline(stages=indexers + [fraud_indexer, assembler, scaler, rf])\n",
    "        model = pipeline.fit(train_data)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model.transform(test_data)\n",
    "\n",
    "        # Evaluate model\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "        auc = evaluator.evaluate(predictions)\n",
    "        print(f\"Run {i+1} - Area Under ROC: {auc}\")\n",
    "\n",
    "        # Confusion Matrix\n",
    "        confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().collect()\n",
    "        tn = next((row[\"count\"] for row in confusion_matrix if row[\"label\"] == 0.0 and row[\"prediction\"] == 0.0), 0)\n",
    "        fp = next((row[\"count\"] for row in confusion_matrix if row[\"label\"] == 0.0 and row[\"prediction\"] == 1.0), 0)\n",
    "        fn = next((row[\"count\"] for row in confusion_matrix if row[\"label\"] == 1.0 and row[\"prediction\"] == 0.0), 0)\n",
    "        tp = next((row[\"count\"] for row in confusion_matrix if row[\"label\"] == 1.0 and row[\"prediction\"] == 1.0), 0)\n",
    "        print(f\"Run {i+1} - Confusion Matrix:\")\n",
    "        print(f\"TN: {tn} | FP: {fp}\")\n",
    "        print(f\"FN: {fn} | TP: {tp}\")\n",
    "\n",
    "        # Calculate additional metrics\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "        # Log parameters and metrics to MLflow\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"seed\", 24)\n",
    "        mlflow.log_metrics({\n",
    "            \"auc\": auc,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": tp,\n",
    "            \"true_negatives\": tn,\n",
    "            \"false_positives\": fp,\n",
    "            \"false_negatives\": fn\n",
    "        })\n",
    "\n",
    "        # ROC Curve\n",
    "        roc_data = predictions.select(\n",
    "            col(\"label\"),\n",
    "            get_class1_prob_udf(col(\"probability\")).alias(\"prob_class1\")\n",
    "        ).orderBy(col(\"prob_class1\").desc())\n",
    "        \n",
    "        total_positives = tp + fn\n",
    "        total_negatives = tn + fp\n",
    "        roc_points = [(0.0, 0.0)]\n",
    "        cum_tp, cum_fp = 0, 0\n",
    "        for row in roc_data.collect():\n",
    "            if row[\"label\"] == 1.0:\n",
    "                cum_tp += 1\n",
    "            else:\n",
    "                cum_fp += 1\n",
    "            tpr = cum_tp / total_positives if total_positives > 0 else 0\n",
    "            fpr = cum_fp / total_negatives if total_negatives > 0 else 0\n",
    "            roc_points.append((fpr, tpr))\n",
    "\n",
    "        # Plot and save ROC curve\n",
    "        fpr, tpr = zip(*roc_points)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\", color=\"blue\")\n",
    "        plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Guess\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"ROC Curve - Run {i+1}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        roc_file = f\"roc_curve_run_{i+1}.png\"\n",
    "        plt.savefig(roc_file)\n",
    "        mlflow.log_artifact(roc_file)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTPUT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
