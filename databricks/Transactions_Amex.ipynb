{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 572424 Amex transactions to 'amex_filtered_transactions.csv'\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Step 1: Load the transactions data\n",
    "# This contains all transaction details\n",
    "df_transactions = pl.read_csv(\"../data/transactions_data.csv\")\n",
    "\n",
    "# Step 2: Load the train_fraud_labels data\n",
    "# This contains transaction_ids we want to keep\n",
    "df_fraud_labels = pl.read_csv(\"../data/train_fraud_labels.csv\")\n",
    "\n",
    "# Step 3: Filter transactions_data to keep only those in train_fraud_labels\n",
    "# Join on 'id' from transactions and 'transaction_id' from fraud labels, keeping only matches 13m -8.9m\n",
    "df_filtered_transactions = df_transactions.join(\n",
    "    df_fraud_labels,\n",
    "    left_on=\"id\",\n",
    "    right_on=\"transaction_id\",\n",
    "    how=\"inner\"  # Only keeps transactions present in both datasets\n",
    ")\n",
    "\n",
    "# Step 4: Load the cards data to get card_brand information\n",
    "df_cards = pl.read_csv(\"../data/cards_data.csv\")\n",
    "\n",
    "# Step 5: Join the filtered transactions with cards data to get card_brand\n",
    "# Join on 'card_id' from transactions and 'id' from cards\n",
    "df_combined = df_filtered_transactions.join(\n",
    "    df_cards,\n",
    "    left_on=\"card_id\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\"  # Left join to keep all transactions. If a card_id doesn't match, it will still be included with null card_brand\n",
    ")\n",
    "\n",
    "# Step 6: Filter for transactions where card_brand is \"Amex\" 8.9m \n",
    "df_amex_transactions = df_combined.filter(pl.col(\"card_brand\") == \"Amex\")\n",
    "\n",
    "# Step 7: Select exactly the specified columns for the output\n",
    "output_df = df_amex_transactions.select([\n",
    "    \"id\",              # Transaction ID\n",
    "    \"date\",           # Transaction date\n",
    "    \"client_id\",      # Client ID\n",
    "    \"card_id\",        # Card ID\n",
    "    \"amount\",         # Transaction amount\n",
    "    \"merchant_id\",    # Merchant ID\n",
    "    \"card_brand\",     # Card brand (will all be Amex)\n",
    "    \"fraud_label\",   # Fraud label from train_fraud_labels\n",
    "    \"mcc\"         # Merchant category code\n",
    "])\n",
    "\n",
    "# Step 8: Save the filtered Amex transactions to a new CSV file\n",
    "output_df.write_csv(\"../data/amex_filtered_transactions.csv\")\n",
    "\n",
    "# Step 9: Print confirmation and row count\n",
    "print(f\"Saved {output_df.height} Amex transactions to 'amex_filtered_transactions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, to_date, when\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Initialize Spark session - This starts our PySpark environment\n",
    "spark = SparkSession.builder.appName(\"DataPreprocessing\").getOrCreate()\n",
    "\n",
    "# Define file locations in cloud storage\n",
    "file_location_cards = \"/FileStore/tables/cards_data.csv\"\n",
    "file_location_users = \"/FileStore/tables/users_data.csv\"\n",
    "file_location_amex = \"/FileStore/tables/amex_filtered_transactions.csv\"\n",
    "\n",
    "# Step 1: Load the datasets from cloud storage\n",
    "# Cards data contains card details like credit limit and PIN change info\n",
    "df_cards = spark.read.csv(file_location_cards, header=True, inferSchema=True)\n",
    "# Users data has user info like income and age\n",
    "df_users = spark.read.csv(file_location_users, header=True, inferSchema=True)\n",
    "# Amex filtered transactions from our previous work\n",
    "df_amex_transactions = spark.read.csv(file_location_amex, header=True, inferSchema=True)\n",
    "\n",
    "# Step 2: Preprocess cards data (kept as DataFrame)\n",
    "# Drop the 'card_on_dark_web' column since we don’t need it\n",
    "df_cards = df_cards.drop(\"card_on_dark_web\")\n",
    "# Clean 'credit_limit' by removing '$' and converting to float\n",
    "df_cards = df_cards.withColumn(\n",
    "    \"credit_limit\",\n",
    "    regexp_replace(col(\"credit_limit\"), \"[\\$,]\", \"\").cast(FloatType())\n",
    ")\n",
    "# Convert 'acct_open_date' to a proper date format (MM/YYYY)\n",
    "df_cards = df_cards.withColumn(\n",
    "    \"acct_open_date\",\n",
    "    to_date(col(\"acct_open_date\"), \"MM/yyyy\")\n",
    ")\n",
    "# Flag if a PIN change is due - if last changed over 7 years ago\n",
    "current_year = 2025  # Using 2025 based on your context\n",
    "df_cards = df_cards.withColumn(\n",
    "    \"PIN_Change_Due\",\n",
    "    when(col(\"year_pin_last_changed\") < current_year - 7, \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "# Step 3: Preprocess users data (kept as DataFrame)\n",
    "# Clean financial columns by removing '$' and converting to float\n",
    "df_users = df_users.withColumn(\n",
    "    \"per_capita_income\",\n",
    "    regexp_replace(col(\"per_capita_income\"), \"[\\$,]\", \"\").cast(FloatType())\n",
    ").withColumn(\n",
    "    \"yearly_income\",\n",
    "    regexp_replace(col(\"yearly_income\"), \"[\\$,]\", \"\").cast(FloatType())\n",
    ").withColumn(\n",
    "    \"total_debt\",\n",
    "    regexp_replace(col(\"total_debt\"), \"[\\$,]\", \"\").cast(FloatType())\n",
    ")\n",
    "# Add 'retirement_status' - Retired if current age meets or exceeds retirement age\n",
    "df_users = df_users.withColumn(\n",
    "    \"retirement_status\",\n",
    "    when(col(\"current_age\") >= col(\"retirement_age\"), \"Retired\").otherwise(\"Not Retired\")\n",
    ")\n",
    "# Categorize 'age_group' based on current age\n",
    "df_users = df_users.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"current_age\") <= 30, \"18-30\")\n",
    "    .when(col(\"current_age\") <= 45, \"31-45\")\n",
    "    .when(col(\"current_age\") <= 60, \"46-60\")\n",
    "    .otherwise(\"60+\")\n",
    ")\n",
    "# Calculate 'Debt_to_Income_Ratio' as total debt divided by yearly income\n",
    "df_users = df_users.withColumn(\n",
    "    \"Debt_to_Income_Ratio\",\n",
    "    col(\"total_debt\") / col(\"yearly_income\")\n",
    ")\n",
    "\n",
    "# Step 4: Preprocess amex_filtered_transactions (kept as DataFrame)\n",
    "# Clean 'amount' by removing '$' and converting to float\n",
    "df_amex_transactions = df_amex_transactions.withColumn(\n",
    "    \"amount\",\n",
    "    regexp_replace(col(\"amount\"), \"[\\$,]\", \"\").cast(FloatType())\n",
    ")\n",
    "\n",
    "# Step 5: Show a preview of the preprocessed DataFrames\n",
    "# Let’s peek at the first few rows to confirm our changes\n",
    "print(\"Preview of preprocessed cards data:\")\n",
    "df_cards.show(5, truncate=False)\n",
    "print(\"Preview of preprocessed users data:\")\n",
    "df_users.show(5, truncate=False)\n",
    "print(\"Preview of preprocessed Amex transactions (with cleaned amount):\")\n",
    "df_amex_transactions.show(5, truncate=False)\n",
    "\n",
    "# Step 6: Count rows and confirm we’re done\n",
    "# We’ll count the rows in each DataFrame to see what we’re working with\n",
    "cards_count = df_cards.count()\n",
    "users_count = df_users.count()\n",
    "amex_count = df_amex_transactions.count()\n",
    "print(f\"Cards DataFrame has {cards_count} rows\")\n",
    "print(f\"Users DataFrame has {users_count} rows\")\n",
    "print(f\"Amex Transactions DataFrame has {amex_count} rows\")\n",
    "\n",
    "# Note: Spark session remains active since we’re keeping DataFrames in memory\n",
    "# Use spark.stop() later when you’re done with these DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
